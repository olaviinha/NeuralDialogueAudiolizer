{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralDialogueAudiolizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SVmriFgdqIU192s-XWAGePsXHOQIFYE1",
      "authorship_tag": "ABX9TyMF9xbHpMmFx8sC1AVVdusO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralInterviewAudiolizer/blob/main/NeuralDialogueAudiolizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5VtWDrgJzuP"
      },
      "source": [
        "<font face=\"Trebuchet MS\" size=\"6\">Neural Dialogue Audiolizer <font color=\"#999\" size=\"3\">Text-to-Audio Dialogue Generator</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralDialogueAudiolizer\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"3\" color=\"#999\"><a href=\"https://inha.se\" target=\"_blank\"><font color=\"#999\">O. Inha</font></a></font></font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6XZgul59e4x"
      },
      "source": [
        "This is a _.txt to .wav converter_ that turns textual dialogue (e.g. an interview, a chat) between two individuals to audio dialogue with two freely selectable voices, currently featuring\n",
        "- [Google Cloud Text-to-Speech API](https://cloud.google.com/text-to-speech) (wavenet voices only)\n",
        "- [Amazon Polly Text-to-Speech API](https://aws.amazon.com/polly/) (neural engine voices only)\n",
        "- [Microsoft Azure Text-to-Speech API](https://azure.microsoft.com/en-us/services/cognitive-services/text-to-speech/) (neural voices only).\n",
        "\n",
        "<font size=\"5\">Note</font>\n",
        "- Access with necessary access keys is required to use any of the provided TTS APIs. See the [Github repository](https://github.com/olaviinha/NeuralDialogueAudiolizer) for more information.\n",
        "- All TTS APIs natively return mono audio with a sample rate of 24 kHz. This notebook converts all results to 44.1 kHz stereo.\n",
        "- At the time of writing this notebook, some packages in Google Colaboratory being deprecated by default. Hence Google Cloud TTS API uses command line interface (curl) instead of Python interface. Will update it to use Python interface later, when it works without extra runtime restarts and hassle.\n",
        "\n",
        "<font size=\"5\">Accepted formats</font>\n",
        "\n",
        ".txt file containing the dialogue must be in one of the following formats.\n",
        "- `question_and_answer` expects an empty line between every time speaker changes.\n",
        "- `dialogue_with_names` expects `Name:` every time speaker changes (e.g. _John: Hello Bob! How are you?_). Speaker is changed despite the name.\n",
        "- For more information and examples, see the [Github repository](https://github.com/olaviinha/NeuralDialogueAudiolizer).\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIwpvYHfN3tC",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import output\n",
        "\n",
        "pip_packages = 'google-api-core google-api-python-client google-auth-httplib2 google-auth-oauthlib google-cloud-texttospeech soundfile boto3 azure-cognitiveservices-speech'\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb'):\n",
        "  %cd /content/\n",
        "  !pip -q install --upgrade import-ipynb {pip_packages}\n",
        "  #!apt-get install sox\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.isdir('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# Drive symlink\n",
        "if not os.path.isdir('/content/mydrive'):\n",
        "  os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "  drive_root_set = True\n",
        "drive_root = '/content/mydrive/'\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "tmp_mono = '/content/tmp/mono/'\n",
        "create_dirs([dir_tmp, tmp_mono])\n",
        "\n",
        "tmp = dir_tmp\n",
        "\n",
        "global_sr = 24000\n",
        "\n",
        "import json, soundfile\n",
        "\n",
        "def writeFile(file, content):\n",
        "  f = open(file, 'w')\n",
        "  f.writelines(content)\n",
        "  f.close()\n",
        "\n",
        "def appendTxt(txt_file, content):\n",
        "  txt = open(txt_file, 'a+') \n",
        "  txt.writelines(content+'\\n')\n",
        "  txt.close()\n",
        "\n",
        "def generate_silence(duration, sr=global_sr):\n",
        "  content = [0]*librosa.time_to_samples(duration, sr=sr)\n",
        "  silence = np.array([content, content], dtype=np.float32)\n",
        "  return silence\n",
        "\n",
        "def save(audio_data, save_as='frank', sr=global_sr):\n",
        "  if save_as=='frank':\n",
        "    global bpm\n",
        "    timestamp = datetime.datetime.today().strftime('%Y%m%d-%H%M%S')\n",
        "    save_as = save_as+'_'+rnd_str(4)+'_'+timestamp+'__'+bpm+'bpm.wav'\n",
        "  soundfile.write(save_as, audio_data.T, sr)\n",
        "\n",
        "# Parse dialogue\n",
        "def parseDialogue(format, dialogue_txt, double_backslash=False):\n",
        "  dlg = []\n",
        "  dialogue = []\n",
        "  aline = ''\n",
        "  if format == 'dialogue_with_names':\n",
        "    with open(dialogue_txt, 'r') as f_in:\n",
        "      dialogue = list(line for line in (l.strip() for l in f_in) if line)\n",
        "    for i, line in enumerate(dialogue):\n",
        "      if ':' in line:\n",
        "        if double_backslash == True:\n",
        "          aline = line.split(':')[1].replace('\\n', '').replace(\"'\", r\"\\\\'\").replace('\\\\\\\\', '\\\\').strip()\n",
        "        else:\n",
        "          aline = line.split(':')[1].replace('\\n', '').strip()\n",
        "        if (i < len(dialogue)-1 and ':' in dialogue[i+1]) or (i >= len(dialogue)-1):\n",
        "          dlg.append(aline)\n",
        "      else:\n",
        "        aline = aline+' '+line\n",
        "        if double_backslash == True:\n",
        "          aline = aline.replace('\\n', '').replace(\"'\", r\"\\\\'\").replace('\\\\\\\\', '\\\\').strip()\n",
        "        else:\n",
        "          aline = aline.replace('\\n', '').strip()\n",
        "        if (i < len(dialogue)-1 and ':' in dialogue[i+1]) or (i >= len(dialogue)-1):\n",
        "          dlg.append(aline)\n",
        "    \n",
        "  if format == 'question_and_answer':\n",
        "    dialogue = open(dialogue_txt, 'r').readlines()\n",
        "    for i, line in enumerate(dialogue):\n",
        "      if len(line) > 1:\n",
        "        aline = aline+' '+line\n",
        "        aline = aline.replace('\\n', '').replace(\"'\", r\"\\\\'\").replace('\\\\\\\\', '\\\\').strip()\n",
        "      else:\n",
        "        dlg.append(aline)\n",
        "        aline = ''\n",
        "    dlg.append(dialogue[-1])\n",
        "  #dialogue = [dlg[::2], dlg[1::2]]\n",
        "  #return [dlg[::2], dlg[1::2]]\n",
        "  return dlg\n",
        "\n",
        "def concat_audio(dir, output_wav):\n",
        "  global global_sr, dialogue_txt\n",
        "  all_audio = []\n",
        "  for audio_file in list_audio(dir):\n",
        "    all_audio.append(librosa.load(audio_file, sr=global_sr, mono=False)[0])\n",
        "  all_audio = np.concatenate(all_audio, axis=1 )\n",
        "  tmp_wav = tmp+path_leaf(output_wav)\n",
        "  save(all_audio, tmp_wav)\n",
        "  !ffmpeg {ffmpeg_q} -y -i \"{tmp_wav}\" {wav_44} \"{output_wav}\"\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'FIN.')\n",
        "\n",
        "# Update voice lists (copy-paste to dropdown)\n",
        "# for voice in en_voices:\n",
        "#   print('\"'+voice[1]+'_'+voice[2]+'\",', end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16ak6XaR8sn8",
        "cellView": "form"
      },
      "source": [
        "#@title #Input dialogue\n",
        "#@markdown <small>Path to .txt file containing the dialogue, located in your Google Drive.</small>\n",
        "dialogue_txt = \"\" #@param {type:\"string\"}\n",
        "format = \"dialogue_with_names\" #@param [\"dialogue_with_names\",\"question_and_answer\"]\n",
        "#@markdown <small>Add this much random variation to the pauses. `pX_pause` sliders determine how long pause there will be **after** that person is done talking.</small>\n",
        "pause_variation = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "dialogue_txt = drive_root+dialogue_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6bA2qunBGoQ",
        "cellView": "form"
      },
      "source": [
        "#@title #Generate audio using Google Cloud TTS\n",
        "\n",
        "#@markdown <small>Path to Google Service Account Key file (json) located in your Google Drive. More info about _how_ [here](https://cloud.google.com/text-to-speech/docs/quickstart-client-libraries#before-you-begin)</small>\n",
        "credentials_file = \"\" #@param {type:\"string\"}\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=drive_root+credentials_file\n",
        "\n",
        "# Fetch voices\n",
        "voices_json = !curl -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) -H \"Content-Type: application/json; charset=utf-8\" \"https://texttospeech.googleapis.com/v1/voices\"\n",
        "voices = json.loads(\"\\n\".join(voices_json))\n",
        "en_voices = []\n",
        "for voice in voices['voices']:\n",
        "  if \"en-\" in voice['languageCodes'][0] and \"Wavenet\" in voice['name']:\n",
        "    en_voices.append([voice['languageCodes'], voice['name'], voice['ssmlGender']])\n",
        "\n",
        "## #@markdown <small>Use cURL instead of Python API.</small>\n",
        "## use_curl = True #@param {type:\"boolean\"}\n",
        "use_curl = True\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #1\n",
        "p1_voice = \"en-US-Wavenet-D_MALE\" #@param [\"en-GB-Wavenet-F_FEMALE\",\"en-IN-Wavenet-D_FEMALE\",\"en-AU-Wavenet-A_FEMALE\",\"en-AU-Wavenet-B_MALE\",\"en-AU-Wavenet-C_FEMALE\",\"en-AU-Wavenet-D_MALE\",\"en-GB-Wavenet-A_FEMALE\",\"en-GB-Wavenet-B_MALE\",\"en-GB-Wavenet-C_FEMALE\",\"en-GB-Wavenet-D_MALE\",\"en-IN-Wavenet-A_FEMALE\",\"en-IN-Wavenet-B_MALE\",\"en-IN-Wavenet-C_MALE\",\"en-US-Wavenet-G_FEMALE\",\"en-US-Wavenet-H_FEMALE\",\"en-US-Wavenet-I_MALE\",\"en-US-Wavenet-J_MALE\",\"en-US-Wavenet-A_MALE\",\"en-US-Wavenet-B_MALE\",\"en-US-Wavenet-C_FEMALE\",\"en-US-Wavenet-D_MALE\",\"en-US-Wavenet-E_FEMALE\",\"en-US-Wavenet-F_FEMALE\",\"fi-FI-Standard-A\",\"fi-FI-Wavenet-A\"]\n",
        "p1_speaking_rate = 1 #@param {type:\"slider\", min:0.25, max:4, step:0.05}\n",
        "p1_pitch = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "p1_pause = 500 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #2\n",
        "p2_voice = \"en-US-Wavenet-F_FEMALE\" #@param [\"en-GB-Wavenet-F_FEMALE\",\"en-IN-Wavenet-D_FEMALE\",\"en-AU-Wavenet-A_FEMALE\",\"en-AU-Wavenet-B_MALE\",\"en-AU-Wavenet-C_FEMALE\",\"en-AU-Wavenet-D_MALE\",\"en-GB-Wavenet-A_FEMALE\",\"en-GB-Wavenet-B_MALE\",\"en-GB-Wavenet-C_FEMALE\",\"en-GB-Wavenet-D_MALE\",\"en-IN-Wavenet-A_FEMALE\",\"en-IN-Wavenet-B_MALE\",\"en-IN-Wavenet-C_MALE\",\"en-US-Wavenet-G_FEMALE\",\"en-US-Wavenet-H_FEMALE\",\"en-US-Wavenet-I_MALE\",\"en-US-Wavenet-J_MALE\",\"en-US-Wavenet-A_MALE\",\"en-US-Wavenet-B_MALE\",\"en-US-Wavenet-C_FEMALE\",\"en-US-Wavenet-D_MALE\",\"en-US-Wavenet-E_FEMALE\",\"en-US-Wavenet-F_FEMALE\",\"fi-FI-Standard-A\",\"fi-FI-Wavenet-A\"]\n",
        "p2_speaking_rate = 1 #@param {type:\"slider\", min:0.25, max:4, step:0.05}\n",
        "p2_pitch = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "p2_pause = 500 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "p1_voice = p1_voice.split('_')[0]\n",
        "p2_voice = p2_voice.split('_')[0]\n",
        "\n",
        "names = []\n",
        "langs = []\n",
        "gends = []\n",
        "for voice in en_voices:\n",
        "  if voice[1] == p1_voice or voice[1] == p2_voice:\n",
        "    names.append(voice[1])\n",
        "    langs.append(voice[0][0])\n",
        "    gends.append(voice[2])\n",
        "\n",
        "dlg = parseDialogue(format, dialogue_txt, True)\n",
        "\n",
        "# Empty tmp\n",
        "if os.listdir(tmp_mono):\n",
        "  !rm {tmp_mono}*\n",
        "if os.listdir(tmp):\n",
        "  !rm {tmp}*\n",
        "\n",
        "# Get audio\n",
        "if use_curl:\n",
        "  for i, repla in enumerate(dlg):\n",
        "    if i % 2 == 0:\n",
        "      # Person 1\n",
        "      vname = names[0]\n",
        "      vlang = langs[0]\n",
        "      vgend = gends[0]\n",
        "      vrate = p1_speaking_rate\n",
        "      vpitc = p1_pitch\n",
        "      pause = p1_pause\n",
        "    else:\n",
        "      # Person 2\n",
        "      vname = names[1]\n",
        "      vlang = langs[1]\n",
        "      vgend = gends[1]\n",
        "      vrate = p2_speaking_rate\n",
        "      vpitc = p2_pitch\n",
        "      pause = p2_pause\n",
        "    input_json = \"{'input':{'text':'\"+str(repla)+\"'},'voice':{'languageCode':'\"+str(vlang)+\"','name':'\"+str(vname)+\"','ssmlGender':'\"+str(vgend)+\"'},'audioConfig':{'speaking_rate': '\"+str(vrate)+\"', 'pitch': '\"+str(vpitc)+\"', 'audioEncoding':'LINEAR16'}}\"\n",
        "    print('\\n')\n",
        "    op(c.title, str(i), repla)\n",
        "    !curl -H \"Authorization: Bearer $(gcloud auth application-default print-access-token)\" -H \"Content-Type: application/json; charset=utf-8\" --data \"{input_json}\" \"https://texttospeech.googleapis.com/v1/text:synthesize\" > temp.json\n",
        "    with open('temp.json') as fp:\n",
        "      for ii, line in enumerate(fp):\n",
        "        if ii == 1:\n",
        "          audio_content = line.replace('\"audioContent\": \"', '').replace('\"', '').strip()\n",
        "    writeFile('base64-temp.txt', audio_content)\n",
        "    tmp_wav_file = tmp_mono+'google_decoded-'+str(i).zfill(4)+'.wav'\n",
        "    wav_file = tmp+'google_decoded-'+str(i).zfill(4)+'.wav'\n",
        "    !base64 base64-temp.txt -d > {tmp_wav_file}\n",
        "    !ffmpeg {ffmpeg_q} -y -i \"{tmp_wav_file}\" {wav_44} \"{wav_file}\"\n",
        "\n",
        "    pvar = pause * pause_variation if odds(0.5) else abs(pause * pause_variation)\n",
        "    pause = pause + pvar\n",
        "    save(generate_silence(pause/1000, sr=44100), tmp+'google_decoded-'+str(i).zfill(4)+'_pause.wav', sr=44100)\n",
        "  \n",
        "  \n",
        "  output_wav = path_dir(dialogue_txt)+basename(dialogue_txt)+'_google_tts_'+rnd_str(4)+'.wav'\n",
        "  concat_audio(tmp, output_wav)\n",
        "\n",
        "  output.clear()\n",
        "  op(c.ok, 'File saved as', output_wav.replace(drive_root, ''))\n",
        "  print('\\n')\n",
        "  audio_player(output_wav)\n",
        "else:\n",
        "  pass\n",
        "  ## Update to Python API whenever it works:\n",
        "  \n",
        "  # from google.cloud import texttospeech\n",
        "  # client = texttospeech.TextToSpeechClient()\n",
        "\n",
        "  # synthesis_input = texttospeech.SynthesisInput(text=\"Hello, World!\")\n",
        "  # voice = texttospeech.VoiceSelectionParams(\n",
        "  #     language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
        "  # )\n",
        "\n",
        "  # audio_config = texttospeech.AudioConfig(\n",
        "  #     audio_encoding=texttospeech.AudioEncoding.LINEAR16\n",
        "  # )\n",
        "\n",
        "  # response = client.synthesize_speech(\n",
        "  #     input=synthesis_input, voice=voice, audio_config=audio_config\n",
        "  # )\n",
        "\n",
        "  # with open('out.wav', \"wb\") as out:\n",
        "  #     out.write(response.audio_content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DNlvGLxoJo",
        "cellView": "form"
      },
      "source": [
        "#@title #Generate audio using Amazon Polly TTS\n",
        "\n",
        "#@markdown <small>You can create access keys [here](https://console.aws.amazon.com/iam/home?#/security_credentials).</small>\n",
        "aws_access_key_id = \"\" #@param {type:\"string\"}\n",
        "aws_secret_access_key = \"\" #@param {type:\"string\"}\n",
        "#region = \"eu-north-1\" #@param {type:\"string\"}\n",
        "region = \"eu-central-1\" #@param [\"eu-central-1\", \"eu-west-2\", \"us-west-2\", \"ap-southeast-1\"]\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #1\n",
        "p1_voice = \"Joey\" #@param [\"Emma\",\"Brian\",\"Ivy\",\"Joanna\",\"Kendra\",\"Kimberly\",\"Salli\",\"Joey\",\"Justin\",\"Kevin\",\"Matthew\"]\n",
        "p1_pause = 500 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #2\n",
        "p2_voice = \"Emma\" #@param [\"Emma\",\"Brian\",\"Ivy\",\"Joanna\",\"Kendra\",\"Kimberly\",\"Salli\",\"Joey\",\"Justin\",\"Kevin\",\"Matthew\"]\n",
        "p2_pause = 500 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "dlg = parseDialogue(format, dialogue_txt)\n",
        "\n",
        "# Empty tmp\n",
        "if os.listdir(tmp_mono):\n",
        "  !rm {tmp_mono}*\n",
        "if os.listdir(tmp):\n",
        "  !rm {tmp}*\n",
        "\n",
        "import boto3\n",
        "polly_client = boto3.Session(aws_access_key_id, aws_secret_access_key, region_name=region).client('polly')\n",
        "\n",
        "for i, repla in enumerate(dlg):\n",
        "\n",
        "  if i % 2 == 0:\n",
        "    # Person 1\n",
        "    voice = p1_voice\n",
        "    pause = p1_pause\n",
        "  else:\n",
        "    # Person 2\n",
        "    voice = p2_voice\n",
        "    pause = p2_pause\n",
        "\n",
        "  op(c.title, str(i), repla)\n",
        "\n",
        "  response = polly_client.synthesize_speech(Engine='neural', VoiceId=voice, OutputFormat='mp3', Text = repla)\n",
        "\n",
        "  mp3_file = tmp+'polly_decoded-'+str(i).zfill(4)+'.mp3'\n",
        "  wav_file = tmp+'polly_decoded-'+str(i).zfill(4)+'.wav'\n",
        "\n",
        "  file = open(mp3_file, 'wb')\n",
        "  file.write(response['AudioStream'].read())\n",
        "  file.close()\n",
        "\n",
        "  pvar = pause * pause_variation if odds(0.5) else abs(pause * pause_variation)\n",
        "  pause = pause + pvar\n",
        "  save(generate_silence(pause/1000), tmp+'polly_decoded-'+str(i).zfill(4)+'_pause.wav')\n",
        "\n",
        "  !ffmpeg {ffmpeg_q} -y -i \"{mp3_file}\" {wav_44} -af \"pan=stereo|c0=c0|c1=c0\" \"{wav_file}\"\n",
        "  !rm \"{mp3_file}\"\n",
        "\n",
        "output_wav = path_dir(dialogue_txt)+basename(dialogue_txt)+'_polly_'+rnd_str(4)+'.wav'\n",
        "concat_audio(tmp, output_wav)\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'File saved as', output_wav.replace(drive_root, ''))\n",
        "print('\\n')\n",
        "audio_player(output_wav)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjSIYRIwdKuQ",
        "cellView": "form"
      },
      "source": [
        "#@title #Generate audio using Microsoft Azure Speech Service\n",
        "key_1 = \"\" #@param {type:\"string\"}\n",
        "#region = \"eu-north-1\" #@param {type:\"string\"}\n",
        "location = \"westeurope\" #@param [\"westeurope\", \"southeastasia\", \"eastus\"]\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #1\n",
        "p1_voice = \"en-GB-RyanNeural\" #@param [\"en-AU-NatashaNeural\",\"en-AU-WilliamNeural\",\"en-CA-ClaraNeural\",\"en-IN-NeerjaNeural\",\"en-IE-EmilyNeural\",\"en-GB-LibbyNeural\",\"en-GB-MiaNeural\",\"en-GB-RyanNeural\",\"en-US-AriaNeural\",\"en-US-GuyNeural\",\"en-US-JennyNeural\",\"fi-FI-NooraNeural\"]\n",
        "p1_pause = 500 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #2\n",
        "p2_voice = \"en-GB-MiaNeural\" #@param [\"en-AU-NatashaNeural\",\"en-AU-WilliamNeural\",\"en-CA-ClaraNeural\",\"en-IN-NeerjaNeural\",\"en-IE-EmilyNeural\",\"en-GB-LibbyNeural\",\"en-GB-MiaNeural\",\"en-GB-RyanNeural\",\"en-US-AriaNeural\",\"en-US-GuyNeural\",\"en-US-JennyNeural\",\"fi-FI-NooraNeural\"]\n",
        "p2_pause = 500 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "dlg = parseDialogue(format, dialogue_txt)\n",
        "\n",
        "# Empty tmp\n",
        "if os.listdir(tmp_mono):\n",
        "  !rm {tmp_mono}*\n",
        "if os.listdir(tmp):\n",
        "  !rm {tmp}*\n",
        "\n",
        "from azure.cognitiveservices.speech import AudioDataStream, SpeechConfig, SpeechSynthesizer, SpeechSynthesisOutputFormat\n",
        "from azure.cognitiveservices.speech.audio import AudioOutputConfig\n",
        "speech_config = SpeechConfig(subscription=key_1, region=location)\n",
        "speech_config.set_speech_synthesis_output_format(SpeechSynthesisOutputFormat[\"Riff24Khz16BitMonoPcm\"])\n",
        "\n",
        "for i, repla in enumerate(dlg):\n",
        "\n",
        "  if i % 2 == 0:\n",
        "    # Person 1\n",
        "    voice = p1_voice\n",
        "    pause = p1_pause\n",
        "  else:\n",
        "    # Person 2\n",
        "    voice = p2_voice\n",
        "    pause = p2_pause\n",
        "\n",
        "  op(c.title, str(i), repla)\n",
        "\n",
        "  speech_config.speech_synthesis_voice_name = voice\n",
        "  synthesizer = SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
        "  result = synthesizer.speak_text_async(repla).get() \n",
        "  stream = AudioDataStream(result)\n",
        "\n",
        "  tmp_wav_file = tmp_mono+'azure_decoded-'+str(i).zfill(4)+'.wav'\n",
        "  wav_file = tmp+'azure_decoded-'+str(i).zfill(4)+'.wav'\n",
        "\n",
        "  stream.save_to_wav_file(tmp_wav_file)\n",
        "  !ffmpeg {ffmpeg_q} -y -i \"{tmp_wav_file}\" {wav_44} -af \"pan=stereo|c0=c0|c1=c0\" \"{wav_file}\"\n",
        "\n",
        "  pvar = pause * pause_variation if odds(0.5) else abs(pause * pause_variation)\n",
        "  pause = pause + pvar\n",
        "  save(generate_silence(pause/1000), tmp+'azure_decoded-'+str(i).zfill(4)+'_pause.wav')\n",
        "\n",
        "output_wav = path_dir(dialogue_txt)+basename(dialogue_txt)+'_azure_'+rnd_str(4)+'.wav'\n",
        "concat_audio(tmp, output_wav)\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'File saved as', output_wav.replace(drive_root, ''))\n",
        "print('\\n')\n",
        "audio_player(output_wav)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}