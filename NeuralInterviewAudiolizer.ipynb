{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralInterviewAudiolizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1SVmriFgdqIU192s-XWAGePsXHOQIFYE1",
      "authorship_tag": "ABX9TyOBEd3v6ajG2DfuSVmwupTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/NeuralInterviewAudiolizer/blob/main/NeuralInterviewAudiolizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5VtWDrgJzuP"
      },
      "source": [
        "#<font face=\"Trebuchet MS\" size=\"6\">Neural Interview Audiolizer <font color=\"#999\" size=\"3\">Text-to-Audio Dialogue Generator</font><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><a href=\"https://github.com/olaviinha/NeuralInterviewAudiolizer\" target=\"_blank\"><font color=\"#999\" size=\"4\">Github</font></a><font color=\"#999\" size=\"4\">&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;</font><font size=\"3\" color=\"#999\"><a href=\"https://inha.se\" target=\"_blank\"><font color=\"#999\">O. Inha</font></a></font></font>\n",
        "\n",
        "This notebook turns a textual dialogue between two individuals (e.g. an interview, a chat) into audio dialogue (.txt to .wav) using either [Google Cloud Text-to-Speech API](https://cloud.google.com/text-to-speech) or [Amazon Polly Text-to-Speech API](https://aws.amazon.com/polly/).\n",
        "\n",
        "<h3>Please note</h3>\n",
        "\n",
        "- Using either of provided APIs **requires access keys**. For more details and instructions on how to obtain the necessary credentials and access, check the following links:\n",
        "  - Google TTS: [Before you begin](https://cloud.google.com/text-to-speech/docs/quickstart-client-libraries#before-you-begin)\n",
        "  - Amazon Polly: [AWS Account and Access Keys](https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html)\n",
        "- Google Cloud TTS API uses command line interface (curl) instead of Python interface due to some packages in Google Colaboratory being deprecated by default, at the time of writing this notebook. Will update it to use Python interface later, when it works without extra runtime restarts, etc.\n",
        "- The returned audio streams are in mono and have native sample rates of around 22-24 kHz. All audio is converted to 44.1 kHz sample rate stereo by this notebook.\n",
        "\n",
        "<h3>Accepted formats</h3>\n",
        "\n",
        ".txt file containing the dialogue must be in one of the following formats. If your dialogue material is a copy-paste from the interwebs, make sure to clean it up first to meet one of the following formats.<br><br>\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "### `dialogue_with_names`\n",
        "\n",
        "In this format, character `:` is used to determine when the person speaking changes. Hence, `:` should not appear anywhere else in the text. Empty lines are ignored. Names (whatever comes before `:` in each line) are ignored.\n",
        "\n",
        "<font color=\"#dd6\">_John: Hello Doe!_<br> _Doe: Well, hello there John._<br>_How are you?_<br>_John: I'm good thank you, and you?_<br>_Doe: Fantastic._</font>\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "### `question_and_answer`\n",
        "\n",
        "In this format, empty lines are used to determine when the person speaking changes, i.e. there must be an empty line in the text after one of the individuals is done talking and the other should start talking.\n",
        "\n",
        "<font color=\"#dd6\">_Hello Doe!_<br><br> _Hi!_<br><br>_How are you?_<br>_What's new?_<br><br>_Struggling as usual, nothing new._<br><br>_What do you think about politics?_<br><br>_Not interested..._<br>_Like no thanks._</font>\n",
        "\n",
        "<hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "<h3>Tips</h3>\n",
        "\n",
        "- You can test what the voices sound like in advance [here](https://cloud.google.com/text-to-speech#section-2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIwpvYHfN3tC",
        "cellView": "form"
      },
      "source": [
        "#@title #Setup\n",
        "#@markdown This cell needs to be run only once. It will mount your Google Drive and setup prerequisites.\n",
        "\n",
        "\n",
        "import os\n",
        "from google.colab import output\n",
        "\n",
        "pip_packages = 'google-api-core google-api-python-client google-auth-httplib2 google-auth-oauthlib google-cloud-texttospeech soundfile boto3'\n",
        "\n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb'):\n",
        "  %cd /content/\n",
        "  !pip -q install --upgrade import-ipynb {pip_packages}\n",
        "  #!apt-get install sox\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "\n",
        "# Mount Drive\n",
        "if not os.path.isdir('/content/drive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "# Drive symlink\n",
        "if not os.path.isdir('/content/mydrive'):\n",
        "  os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "  drive_root_set = True\n",
        "drive_root = '/content/mydrive/'\n",
        "\n",
        "dir_tmp = '/content/tmp/'\n",
        "tmp_mono = '/content/tmp/mono/'\n",
        "create_dirs([dir_tmp, tmp_mono])\n",
        "\n",
        "tmp = dir_tmp\n",
        "\n",
        "global_sr = 24000\n",
        "\n",
        "import json, soundfile\n",
        "\n",
        "def writeFile(file, content):\n",
        "  f = open(file, 'w')\n",
        "  f.writelines(content)\n",
        "  f.close()\n",
        "\n",
        "def appendTxt(txt_file, content):\n",
        "  txt = open(txt_file, 'a+') \n",
        "  txt.writelines(content+'\\n')\n",
        "  txt.close()\n",
        "\n",
        "def generate_silence(duration, sr=global_sr):\n",
        "  content = [0]*librosa.time_to_samples(duration, sr=sr)\n",
        "  silence = np.array([content, content], dtype=np.float32)\n",
        "  return silence\n",
        "\n",
        "def save(audio_data, save_as='frank', sr=global_sr):\n",
        "  if save_as=='frank':\n",
        "    global bpm\n",
        "    timestamp = datetime.datetime.today().strftime('%Y%m%d-%H%M%S')\n",
        "    save_as = save_as+'_'+rnd_str(4)+'_'+timestamp+'__'+bpm+'bpm.wav'\n",
        "  soundfile.write(save_as, audio_data.T, sr)\n",
        "\n",
        "# Parse dialogue\n",
        "def parseDialogue(format, dialogue_txt, double_backslash=False):\n",
        "  dlg = []\n",
        "  dialogue = []\n",
        "  aline = ''\n",
        "  if format == 'dialogue_with_names':\n",
        "    with open(dialogue_txt, 'r') as f_in:\n",
        "      dialogue = list(line for line in (l.strip() for l in f_in) if line)\n",
        "    for i, line in enumerate(dialogue):\n",
        "      if ':' in line:\n",
        "        if double_backslash == True:\n",
        "          aline = line.split(':')[1].replace('\\n', '').replace(\"'\", r\"\\\\'\").replace('\\\\\\\\', '\\\\').strip()\n",
        "        else:\n",
        "          aline = line.split(':')[1].replace('\\n', '').strip()\n",
        "        if (i < len(dialogue)-1 and ':' in dialogue[i+1]) or (i >= len(dialogue)-1):\n",
        "          dlg.append(aline)\n",
        "      else:\n",
        "        aline = aline+' '+line\n",
        "        if double_backslash == True:\n",
        "          aline = aline.replace('\\n', '').replace(\"'\", r\"\\\\'\").replace('\\\\\\\\', '\\\\').strip()\n",
        "        else:\n",
        "          aline = aline.replace('\\n', '').strip()\n",
        "        if (i < len(dialogue)-1 and ':' in dialogue[i+1]) or (i >= len(dialogue)-1):\n",
        "          dlg.append(aline)\n",
        "    \n",
        "  if format == 'question_and_answer':\n",
        "    dialogue = open(dialogue_txt, 'r').readlines()\n",
        "    for i, line in enumerate(dialogue):\n",
        "      if len(line) > 1:\n",
        "        aline = aline+' '+line\n",
        "        aline = aline.replace('\\n', '').replace(\"'\", r\"\\\\'\").replace('\\\\\\\\', '\\\\').strip()\n",
        "      else:\n",
        "        dlg.append(aline)\n",
        "        aline = ''\n",
        "    dlg.append(dialogue[-1])\n",
        "  #dialogue = [dlg[::2], dlg[1::2]]\n",
        "  #return [dlg[::2], dlg[1::2]]\n",
        "  return dlg\n",
        "\n",
        "def concat_audio(dir, output_wav):\n",
        "  global global_sr, dialogue_txt\n",
        "  all_audio = []\n",
        "  for audio_file in list_audio(dir):\n",
        "    all_audio.append(librosa.load(audio_file, sr=global_sr, mono=False)[0])\n",
        "  all_audio = np.concatenate(all_audio, axis=1)\n",
        "  tmp_wav = tmp+path_leaf(output_wav)\n",
        "  save(all_audio, tmp_wav)\n",
        "  !ffmpeg {ffmpeg_q} -y -i \"{tmp_wav}\" {wav_44} \"{output_wav}\"\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'FIN.')\n",
        "\n",
        "# Update voice lists (copy-paste to dropdown)\n",
        "# for voice in en_voices:\n",
        "#   print('\"'+voice[1]+'_'+voice[2]+'\",', end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6bA2qunBGoQ",
        "cellView": "form"
      },
      "source": [
        "#@title #Google Cloud TTS\n",
        "\n",
        "#@markdown <small>Path to Google Service Account Key file (json) located in your Google Drive. More info about _how_ [here](https://cloud.google.com/text-to-speech/docs/quickstart-client-libraries#before-you-begin)</small>\n",
        "credentials_file = \"\" #@param {type:\"string\"}\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=drive_root+credentials_file\n",
        "\n",
        "# Fetch voices\n",
        "voices_json = !curl -H \"Authorization: Bearer \"$(gcloud auth application-default print-access-token) -H \"Content-Type: application/json; charset=utf-8\" \"https://texttospeech.googleapis.com/v1/voices\"\n",
        "voices = json.loads(\"\\n\".join(voices_json))\n",
        "en_voices = []\n",
        "for voice in voices['voices']:\n",
        "  if \"en-\" in voice['languageCodes'][0] and \"Wavenet\" in voice['name']:\n",
        "    en_voices.append([voice['languageCodes'], voice['name'], voice['ssmlGender']])\n",
        "\n",
        "\n",
        "#@markdown <small>Path to .txt file containing the dialogue, located in your Google Drive.</small>\n",
        "dialogue_txt = \"\" #@param {type:\"string\"}\n",
        "format = \"dialogue_with_names\" #@param [\"dialogue_with_names\",\"question_and_answer\"]\n",
        "#@markdown <small>Add this much random variation to the pauses. `pX_pause` sliders determine how long pause there will be **after** that person is done talking.</small>\n",
        "pause_variation = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "## #@markdown <small>Use cURL instead of Python API.</small>\n",
        "## use_curl = True #@param {type:\"boolean\"}\n",
        "use_curl = True\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #1\n",
        "p1_voice = \"en-US-Wavenet-B_MALE\" #@param [\"en-GB-Wavenet-F_FEMALE\",\"en-IN-Wavenet-D_FEMALE\",\"en-AU-Wavenet-A_FEMALE\",\"en-AU-Wavenet-B_MALE\",\"en-AU-Wavenet-C_FEMALE\",\"en-AU-Wavenet-D_MALE\",\"en-GB-Wavenet-A_FEMALE\",\"en-GB-Wavenet-B_MALE\",\"en-GB-Wavenet-C_FEMALE\",\"en-GB-Wavenet-D_MALE\",\"en-IN-Wavenet-A_FEMALE\",\"en-IN-Wavenet-B_MALE\",\"en-IN-Wavenet-C_MALE\",\"en-US-Wavenet-G_FEMALE\",\"en-US-Wavenet-H_FEMALE\",\"en-US-Wavenet-I_MALE\",\"en-US-Wavenet-J_MALE\",\"en-US-Wavenet-A_MALE\",\"en-US-Wavenet-B_MALE\",\"en-US-Wavenet-C_FEMALE\",\"en-US-Wavenet-D_MALE\",\"en-US-Wavenet-E_FEMALE\",\"en-US-Wavenet-F_FEMALE\"]\n",
        "p1_speaking_rate = 1 #@param {type:\"slider\", min:0.25, max:4, step:0.05}\n",
        "p1_pitch = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "p1_pause = 10 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #2\n",
        "p2_voice = \"en-GB-Wavenet-C_FEMALE\" #@param [\"en-GB-Wavenet-F_FEMALE\",\"en-IN-Wavenet-D_FEMALE\",\"en-AU-Wavenet-A_FEMALE\",\"en-AU-Wavenet-B_MALE\",\"en-AU-Wavenet-C_FEMALE\",\"en-AU-Wavenet-D_MALE\",\"en-GB-Wavenet-A_FEMALE\",\"en-GB-Wavenet-B_MALE\",\"en-GB-Wavenet-C_FEMALE\",\"en-GB-Wavenet-D_MALE\",\"en-IN-Wavenet-A_FEMALE\",\"en-IN-Wavenet-B_MALE\",\"en-IN-Wavenet-C_MALE\",\"en-US-Wavenet-G_FEMALE\",\"en-US-Wavenet-H_FEMALE\",\"en-US-Wavenet-I_MALE\",\"en-US-Wavenet-J_MALE\",\"en-US-Wavenet-A_MALE\",\"en-US-Wavenet-B_MALE\",\"en-US-Wavenet-C_FEMALE\",\"en-US-Wavenet-D_MALE\",\"en-US-Wavenet-E_FEMALE\",\"en-US-Wavenet-F_FEMALE\"]\n",
        "p2_speaking_rate = 1 #@param {type:\"slider\", min:0.25, max:4, step:0.05}\n",
        "p2_pitch = 0 #@param {type:\"slider\", min:-20, max:20, step:1}\n",
        "p2_pause = 70 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "\n",
        "\n",
        "dialogue_txt = drive_root+dialogue_txt\n",
        "p1_voice = p1_voice.split('_')[0]\n",
        "p2_voice = p2_voice.split('_')[0]\n",
        "\n",
        "names = []\n",
        "langs = []\n",
        "gends = []\n",
        "for voice in en_voices:\n",
        "  if voice[1] == p1_voice or voice[1] == p2_voice:\n",
        "    names.append(voice[1])\n",
        "    langs.append(voice[0][0])\n",
        "    gends.append(voice[2])\n",
        "\n",
        "dlg = parseDialogue(format, dialogue_txt, True)\n",
        "\n",
        "# Empty tmp\n",
        "if os.listdir(tmp_mono):\n",
        "  !rm {tmp_mono}*\n",
        "if os.listdir(tmp):\n",
        "  !rm {tmp}*\n",
        "\n",
        "# Get audio\n",
        "if use_curl:\n",
        "  for i, repla in enumerate(dlg):\n",
        "    if i % 2 == 0:\n",
        "      # Person 2\n",
        "      vname = names[1]\n",
        "      vlang = langs[1]\n",
        "      vgend = gends[1]\n",
        "      vrate = p2_speaking_rate\n",
        "      vpitc = p2_pitch\n",
        "      pause = p2_pause\n",
        "    else:\n",
        "      # Person 1\n",
        "      vname = names[0]\n",
        "      vlang = langs[0]\n",
        "      vgend = gends[0]\n",
        "      vrate = p1_speaking_rate\n",
        "      vpitc = p1_pitch\n",
        "      pause = p1_pause\n",
        "    input_json = \"{'input':{'text':'\"+str(repla)+\"'},'voice':{'languageCode':'\"+str(vlang)+\"','name':'\"+str(vname)+\"','ssmlGender':'\"+str(vgend)+\"'},'audioConfig':{'speaking_rate': '\"+str(vrate)+\"', 'pitch': '\"+str(vpitc)+\"', 'audioEncoding':'LINEAR16'}}\"\n",
        "    print('\\n')\n",
        "    op(c.title, str(i), repla)\n",
        "    !curl -H \"Authorization: Bearer $(gcloud auth application-default print-access-token)\" -H \"Content-Type: application/json; charset=utf-8\" --data \"{input_json}\" \"https://texttospeech.googleapis.com/v1/text:synthesize\" > temp.json\n",
        "    with open('temp.json') as fp:\n",
        "      for ii, line in enumerate(fp):\n",
        "        if ii == 1:\n",
        "          audio_content = line.replace('\"audioContent\": \"', '').replace('\"', '').strip()\n",
        "    writeFile('base64-temp.txt', audio_content)\n",
        "    tmp_wav_file = tmp_mono+'google_decoded-'+str(i).zfill(4)+'.wav'\n",
        "    wav_file = tmp+'google_decoded-'+str(i).zfill(4)+'.wav'\n",
        "    !base64 base64-temp.txt -d > {tmp_wav_file}\n",
        "    !ffmpeg {ffmpeg_q} -y -i \"{tmp_wav_file}\" {wav_44} \"{wav_file}\"\n",
        "\n",
        "    pvar = pause * pause_variation if odds(0.5) else abs(pause * pause_variation)\n",
        "    pause = pause + pvar\n",
        "    save(generate_silence(pause/1000, sr=44100), tmp+'google_decoded-'+str(i).zfill(4)+'_pause.wav', sr=44100)\n",
        "  \n",
        "  \n",
        "  output_wav = path_dir(dialogue_txt)+basename(dialogue_txt)+'_google_tts_'+rnd_str(4)+'.wav'\n",
        "  concat_audio(tmp, output_wav)\n",
        "\n",
        "  output.clear()\n",
        "  op(c.ok, 'File saved as', output_wav.replace(drive_root, ''))\n",
        "  print('\\n')\n",
        "  audio_player(output_wav)\n",
        "else:\n",
        "  pass\n",
        "  ## Update to Python API whenever it works:\n",
        "  \n",
        "  # from google.cloud import texttospeech\n",
        "  # client = texttospeech.TextToSpeechClient()\n",
        "\n",
        "  # synthesis_input = texttospeech.SynthesisInput(text=\"Hello, World!\")\n",
        "  # voice = texttospeech.VoiceSelectionParams(\n",
        "  #     language_code=\"en-US\", ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n",
        "  # )\n",
        "\n",
        "  # audio_config = texttospeech.AudioConfig(\n",
        "  #     audio_encoding=texttospeech.AudioEncoding.LINEAR16\n",
        "  # )\n",
        "\n",
        "  # response = client.synthesize_speech(\n",
        "  #     input=synthesis_input, voice=voice, audio_config=audio_config\n",
        "  # )\n",
        "\n",
        "  # with open('out.wav', \"wb\") as out:\n",
        "  #     out.write(response.audio_content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_DNlvGLxoJo",
        "cellView": "form"
      },
      "source": [
        "#@title #Amazon Polly TTS\n",
        "\n",
        "#@markdown <small>You can create access keys [here](https://console.aws.amazon.com/iam/home?#/security_credentials).</small>\n",
        "aws_access_key_id = \"\" #@param {type:\"string\"}\n",
        "aws_secret_access_key = \"\" #@param {type:\"string\"}\n",
        "#region = \"eu-north-1\" #@param {type:\"string\"}\n",
        "region = \"eu-central-1\" #@param [\"us-east-1\", \"us-east-2\", \"us-west-1\", \"us-west-2\", \"eu-central-1\", \"eu-west-2\"]\n",
        "\n",
        "\n",
        "\n",
        "#@markdown <small>Path to .txt file containing the dialogue, located in your Google Drive.</small>\n",
        "dialogue_txt = \"\" #@param {type:\"string\"}\n",
        "format = \"dialogue_with_names\" #@param [\"dialogue_with_names\",\"question_and_answer\"]\n",
        "#@markdown <small>Add this much random variation to the pauses. `pX_pause` sliders determine how long pause there will be **after** that person is done talking.</small>\n",
        "pause_variation = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "## #@markdown <small>Use cURL instead of Python API.</small>\n",
        "## use_curl = True #@param {type:\"boolean\"}\n",
        "use_curl = True\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #1\n",
        "p1_voice = \"Brian\" #@param [\"Emma\",\"Brian\",\"Ivy\",\"Joanna\",\"Kendra\",\"Kimberly\",\"Salli\",\"Joey\",\"Justin\",\"Kevin\",\"Matthew\"]\n",
        "p1_pause = 10 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "#@markdown ###Individual #2\n",
        "p2_voice = \"Joanna\" #@param [\"Emma\",\"Brian\",\"Ivy\",\"Joanna\",\"Kendra\",\"Kimberly\",\"Salli\",\"Joey\",\"Justin\",\"Kevin\",\"Matthew\"]\n",
        "p2_pause = 70 #@param {type:\"slider\", min:0, max:2000, step:10}\n",
        "\n",
        "#@markdown <hr size=\"1\" color=\"#666\"/>\n",
        "\n",
        "\n",
        "\n",
        "dialogue_txt = drive_root+dialogue_txt\n",
        "\n",
        "dlg = parseDialogue(format, dialogue_txt)\n",
        "\n",
        "# Empty tmp\n",
        "if os.listdir(tmp):\n",
        "  !rm {tmp}*\n",
        "\n",
        "import boto3\n",
        "polly_client = boto3.Session(aws_access_key_id, aws_secret_access_key, region_name=region).client('polly')\n",
        "\n",
        "for i, repla in enumerate(dlg):\n",
        "\n",
        "  if i % 2 == 0:\n",
        "    # Person 2\n",
        "    voice = p1_voice\n",
        "    pause = p1_pause\n",
        "  else:\n",
        "    # Person 1\n",
        "    voice = p2_voice\n",
        "    pause = p2_pause\n",
        "\n",
        "  op(c.title, str(i), repla)\n",
        "\n",
        "  response = polly_client.synthesize_speech(Engine='neural', VoiceId=voice, OutputFormat='mp3', Text = repla)\n",
        "\n",
        "  mp3_file = tmp+'polly_decoded-'+str(i).zfill(4)+'.mp3'\n",
        "  wav_file = tmp+'polly_decoded-'+str(i).zfill(4)+'.wav'\n",
        "\n",
        "  file = open(mp3_file, 'wb')\n",
        "  file.write(response['AudioStream'].read())\n",
        "  file.close()\n",
        "\n",
        "  pvar = pause * pause_variation if odds(0.5) else abs(pause * pause_variation)\n",
        "  pause = pause + pvar\n",
        "  save(generate_silence(pause/1000), tmp+'polly_decoded-'+str(i).zfill(4)+'_pause.wav')\n",
        "\n",
        "  !ffmpeg {ffmpeg_q} -y -i \"{mp3_file}\" {wav_44} -af \"pan=stereo|c0=c0|c1=c0\" \"{wav_file}\"\n",
        "  !rm \"{mp3_file}\"\n",
        "\n",
        "output_wav = path_dir(dialogue_txt)+basename(dialogue_txt)+'_polly_'+rnd_str(4)+'.wav'\n",
        "concat_audio(tmp, output_wav)\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'File saved as', output_wav.replace(drive_root, ''))\n",
        "print('\\n')\n",
        "audio_player(output_wav)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}